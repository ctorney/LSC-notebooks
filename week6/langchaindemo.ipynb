{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbf3f3-2960-4d6f-8fd9-a73208e06648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9e3d385-f069-487d-84cb-1394014e2076",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install ollama - local language models\n",
    "\n",
    "Note, we need to install ollama and langchain with the default environment\n",
    "\n",
    "(see here for details https://docs.aws.amazon.com/sagemaker/latest/dg/studio-lab-environments.html)\n",
    "\n",
    "Langchain is installed on the persistent environment but it is an older version\n",
    "\n",
    "Some libraries for text integrating langchain with ollama\n",
    "\n",
    "https://pypi.org/project/langchain-community/\n",
    "\n",
    "https://github.com/chroma-core/chroma\n",
    "\n",
    "ChromaDB, often referred to simply as Chroma, is an open-source embedding database designed to store, query, and manage embeddings efficiently. Embeddings are numerical representations of data, typically derived from machine learning models, that capture semantic information in a high-dimensional space. They are widely used in applications like natural language processing, recommendation systems, and image recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1c7ec9-7584-455d-8096-936f1e7f3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tmp\n",
    "print(\"Downloading ollama...\")\n",
    "!curl --fail --show-error --location --progress-bar -o tmp/ollama \"https://ollama.com/download/ollama-linux-amd64\"\n",
    "print(\"Installing ollama...\")\n",
    "!install -m755 tmp/ollama /home/studio-lab-user/.conda/envs/default/bin/ollama\n",
    "print(\"Cleaning up...\")\n",
    "!rm -rf tmp\n",
    "print(\"Upgrading langchain to work with latest models...\")\n",
    "%pip install -U langchain-community chromadb --quiet\n",
    "print(\"Install complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b3a6d-c4ff-4032-a4ca-c7eb6255aa41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dd1ae68-40ee-498c-887c-fca3538d3a2b",
   "metadata": {},
   "source": [
    "## Next we'll launch ollama. \n",
    "\n",
    "Note all this code does is run `ollama serve` but it uses threading so that we don't block the notebook and can keep running other cells. You can also run the command in a separate terminal to achieve the same effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc653e5-eda1-4dd7-80eb-988b024fa06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "\n",
    "async def run_process(cmd):\n",
    "    print('>>> starting', *cmd)\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *cmd,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    \n",
    "async def start_ollama_serve():\n",
    "    await run_process(['ollama', 'serve'])\n",
    "\n",
    "def run_async_in_thread(loop, coro):\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(coro) \n",
    "    loop.close()\n",
    "\n",
    "# Create a new event loop that will run in a new thread \n",
    "new_loop = asyncio.new_event_loop() \n",
    "\n",
    "# Start ollama serve in a separate thread so the cell won't block execution \n",
    "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
    "thread.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8ec49-05e7-4a28-bdce-ac73ea4e09b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\",  base_url='http://localhost:11434')\n",
    "\n",
    "print(llm.invoke(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be8cfc-b84a-4730-aa00-f7edfc89a46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llm.invoke(\"Tell me another joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269adac9-e14b-4035-ac72-1531916f8b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llm.invoke(\"Tell me a joke about AI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099955c6-1d8f-4eb5-b2b5-96ae5f55f0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(llm.invoke(\"why is the sky blue\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609a62a-0ed8-4044-b6be-616b27707fbf",
   "metadata": {},
   "source": [
    "## Working with text data (RAG - Retrieval-Augmented Generation)\n",
    "\n",
    "https://ollama.com/blog/embedding-models\n",
    "\n",
    "\n",
    "In Langchain, a VectorStoreIndexCreator is a module or component responsible for creating an index for storing and retrieving vectors efficiently. Vectors represent numerical data points in multi-dimensional space and are commonly used in data analysis, machine learning, and natural language processing tasks. The VectorStoreIndexCreator helps organize these vectors in a way that allows for quick and effective searching and querying.\n",
    "\n",
    "The vectorstore as_retriever is setting up a retriever to perform approximate nearest neighbor search using a vector store called `index.vectorstore`. The `as_retriever` method is being used to convert the vector store into a retriever object. The `search_kwargs={\"k\": 1000}` parameter specifies that the retriever should return the top 1000 nearest neighbors for the query vectors. The retriever object allows you to efficiently search for similar vectors within the vector store by retrieving the nearest neighbors based on similarity metrics like cosine similarity or Euclidean distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c612829-643a-4260-b02a-4e11ab841918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "course_codes = ['STATS5073',\n",
    "                'STATS5074',\n",
    "                'STATS5075',\n",
    "                'STATS5076',\n",
    "                'STATS5077',\n",
    "                'STATS5078',\n",
    "                'STATS5079',\n",
    "                'STATS5080',\n",
    "                'STATS5081',\n",
    "                'STATS5082',\n",
    "                'STATS5083',\n",
    "                'STATS5084']\n",
    "\n",
    "base_url = \"https://www.gla.ac.uk/postgraduate/taught/dataanalyticsonline/?card=course&code=\"\n",
    "\n",
    "course_list = [base_url + c for c in course_codes]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb127259-7984-4dc4-bb16-13c4368d934c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "course_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07406007-9899-4c71-8b18-9e7b2142b61c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955090f6-f742-46de-88ab-032af66fe91a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "\n",
    "loader = WebBaseLoader(web_paths=course_list[0:12])\n",
    "index = VectorstoreIndexCreator(embedding=oembed).from_loaders([loader])\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "  llm=llm,\n",
    "  retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 10}),\n",
    ")\n",
    "\n",
    "chat_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c312d-aa8f-4658-bf25-7f8e20330113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Summarize the combined content of all the courses. Group similar topics together\"\n",
    "result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "\n",
    "chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0f8d3-f751-4998-881c-bc79eae0e99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Summarize the deep learning components of the programme \"\n",
    "result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "\n",
    "chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6905773-66aa-4af3-9b03-14bedf080a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60c319-fe06-4fde-b5a3-75a461367756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f60ad72-e1c7-4cf0-9083-db862cc7d029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mostpopularpage = ['https://en.wikipedia.org/wiki/Bill_Walton']\n",
    "\n",
    "\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "\n",
    "loader = WebBaseLoader(web_paths=mostpopularpage)\n",
    "index = VectorstoreIndexCreator(embedding=oembed).from_loaders([loader])\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "  llm=llm,\n",
    "  retriever=index.vectorstore.as_retriever(search_kwargs={\"k\": 10}),\n",
    ")\n",
    "\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720c5df-3188-40d8-a002-039ee2dcbf9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Where was Bill Walton born?\"\n",
    "result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "\n",
    "chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aba76e-0eba-4e71-9670-8f0355b2cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Are you sure you don't have information about his early life?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe6e35-ec20-4ec6-8c24-9265f32c202f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "\n",
    "chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169319ad-52a2-43d5-910e-37a5bacf3a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083de18c-c8d5-4ac3-a8d9-36f2abfc0761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883bb4c3-d901-4722-8dd9-e9b15ed9e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3\"\n",
    ")\n",
    "print(ollama.invoke(\"why is the sky blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dff169-e849-4f15-90e3-743c140e0920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
